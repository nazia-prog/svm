{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is a Support Vector Machine (SVM), and how does it work?\n",
        "\n",
        "ans- A Support Vector Machine (SVM) is a powerful supervised machine learning algorithm used for classification and regression tasks, especially effective in high-dimensional spaces.\n",
        "\n",
        "‚öôÔ∏è How Does SVM Work?\n",
        "At its core, SVM tries to find the best decision boundary (called a hyperplane) that separates data points of different classes with the maximum margin.\n",
        "Here's the step-by-step intuition:\n",
        "- Plot the data in n-dimensional space (where n = number of features).\n",
        "- Identify the hyperplane that best separates the classes.\n",
        "- Maximize the margin ‚Äî the distance between the hyperplane and the nearest data points from each class (called support vectors).\n",
        "- If data isn‚Äôt linearly separable, SVM uses:\n",
        "- Kernel trick to transform data into higher dimensions.\n",
        "- Common kernels: linear, polynomial, RBF (Gaussian).\n",
        "\n",
        "Question 2: Explain the difference between Hard Margin and Soft Margin SVM.\n",
        "\n",
        "ans- Hard Margin SVM\n",
        "Definition: Assumes that the data is perfectly linearly separable ‚Äî meaning there exists a hyperplane that separates the classes without any misclassification.\n",
        "Key Characteristics:\n",
        "- No tolerance for errors or overlap.\n",
        "- Maximizes the margin strictly between classes.\n",
        "- Very sensitive to outliers ‚Äî even one misclassified point can break the model.\n",
        "Use Case: Rare in practice; mostly theoretical or for clean, synthetic datasets.\n",
        "\n",
        "üßä Soft Margin SVM\n",
        "Definition: Allows for some misclassification by introducing a penalty term for incorrectly classified points. Balances margin maximization with classification error.\n",
        "Key Characteristics:\n",
        "- Introduces a regularization parameter C:\n",
        "- Low C ‚Üí wider margin, more tolerance for misclassification.\n",
        "- High C ‚Üí narrower margin, less tolerance for errors.\n",
        "- More robust to noisy data and outliers.\n",
        "- Widely used in real-world applications.\n",
        "Use Case: Ideal for datasets with overlapping classes or noise ‚Äî like medical diagnosis, text classification, etc.\n",
        "\n",
        "\n",
        "Question 3: What is the Kernel Trick in SVM? Give one example of a kernel and explain its use case.\n",
        "\n",
        "ans-The Kernel Trick allows SVMs to operate in a higher-dimensional space without explicitly computing the transformation. Instead of mapping data points to a new space, it computes the dot product between them in that space using a kernel function.\n",
        "This enables SVMs to find a linear decision boundary in a transformed space, which corresponds to a non-linear boundary in the original space.\n",
        "\n",
        "üìå Why Is It Useful?\n",
        "- Avoids the computational cost of explicitly transforming data.\n",
        "- Makes SVMs capable of handling complex, non-linear relationships.\n",
        "- Keeps the algorithm efficient even in very high-dimensional spaces.\n",
        "\n",
        "üåü Example: Radial Basis Function (RBF) Kernel\n",
        "Definition:\n",
        "K(x, x') = \\exp\\left(-\\gamma \\|x - x'\\|^2\\right)\n",
        "Use Case:\n",
        "- Ideal for datasets where the decision boundary is non-linear and complex.\n",
        "- Commonly used in image classification, bioinformatics, and medical diagnosis where patterns are not linearly separable.\n",
        "Intuition:\n",
        "The RBF kernel measures similarity between two points. If they‚Äôre close, the kernel value is high; if they‚Äôre far apart, it‚Äôs low. This helps the SVM focus on local patterns.\n",
        "\n",
        "Question 4: What is a Na√Øve Bayes Classifier, and why is it called ‚Äúna√Øve‚Äù?\n",
        "\n",
        "ans-  What Is a Na√Øve Bayes Classifier?\n",
        "The Na√Øve Bayes Classifier is a probabilistic machine learning algorithm based on Bayes‚Äô Theorem. It‚Äôs used for classification tasks and is especially popular in text classification (like spam detection, sentiment analysis, etc.).\n",
        "Bayes‚Äô Theorem:\n",
        "P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n",
        "In the context of classification:\n",
        "P(\\text{Class}|\\text{Features}) = \\frac{P(\\text{Features}|\\text{Class}) \\cdot P(\\text{Class})}{P(\\text{Features})}\n",
        "\n",
        "ü§ì Why Is It Called ‚ÄúNa√Øve‚Äù?\n",
        "It‚Äôs called na√Øve because it makes a strong assumption:\n",
        "All features are independent of each other given the class label.\n",
        "\n",
        "This assumption is rarely true in real-world data (e.g., in text, words often co-occur), but the model still performs surprisingly well in many cases.\n",
        "\n",
        "Question 5: Describe the Gaussian, Multinomial, and Bernoulli Na√Øve Bayes variants. When would you use each one?\n",
        "\n",
        "ans-  1. Gaussian Na√Øve Bayes\n",
        "Assumption: Features follow a normal (Gaussian) distribution.\n",
        "Use Case:\n",
        "- Best for continuous numerical data like age, blood pressure, or income.\n",
        "- Common in medical diagnosis, sensor data, and regression-style classification.\n",
        "Example: Predicting disease based on lab test values (e.g., glucose levels, cholesterol).\n",
        "\n",
        "üìä 2. Multinomial Na√Øve Bayes\n",
        "Assumption: Features represent discrete counts (e.g., word frequencies).\n",
        "Use Case:\n",
        "- Ideal for text classification, document categorization, and spam detection.\n",
        "- Works well with Bag-of-Words or TF-IDF representations.\n",
        "Example: Classifying emails as spam or not based on word occurrence.\n",
        "\n",
        "üßÆ 3. Bernoulli Na√Øve Bayes\n",
        "Assumption: Features are binary (0 or 1), indicating presence or absence.\n",
        "Use Case:\n",
        "- Suitable for binary feature vectors, like whether a word appears in a document.\n",
        "- Useful when you care about presence, not frequency.\n",
        "Example: Sentiment analysis using binary word presence (e.g., ‚Äúhappy‚Äù = 1 if present).\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "IIaYzw31bx46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 6: Write a Python program to: ‚óè Load the Iris dataset ‚óè Train an SVM Classifier with a linear kernel ‚óè Print the model's accuracy and support vectors.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train an SVM Classifier with a linear kernel\n",
        "svm_model = SVC(kernel='linear', random_state=42)\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate accuracy\n",
        "y_pred = svm_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Print support vectors\n",
        "print(\"\\nSupport Vectors:\")\n",
        "print(svm_model.support_vectors_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNyvws8ze6JM",
        "outputId": "33c711fb-f06a-4991-9387-cffb6f83314d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.00\n",
            "\n",
            "Support Vectors:\n",
            "[[4.8 3.4 1.9 0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.7 3.  5.  1.7]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [5.9 3.  5.1 1.8]\n",
            " [4.9 2.5 4.5 1.7]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 7: Write a Python program to:\n",
        "# Load the Breast Cancer dataset\n",
        "# Train a Gaussian Na√Øve Bayes model\n",
        "# Print its classification report including precision, recall, and F1-score.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Gaussian Na√Øve Bayes model\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = gnb.predict(X_test)\n",
        "report = classification_report(y_test, y_pred, target_names=data.target_names)\n",
        "print(\"Classification Report:\\n\")\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyS0Al4ffMvs",
        "outputId": "c3126abe-59d7-43e9-9013-c48616cef8c0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       0.93      0.90      0.92        63\n",
            "      benign       0.95      0.96      0.95       108\n",
            "\n",
            "    accuracy                           0.94       171\n",
            "   macro avg       0.94      0.93      0.94       171\n",
            "weighted avg       0.94      0.94      0.94       171\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 8: Write a Python program to:\n",
        "# Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best C and gamma.\n",
        "#Print the best hyperparameters and accuracy.\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define parameter grid for C and gamma\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [0.001, 0.01, 0.1, 1],\n",
        "    'kernel': ['rbf']  # Using RBF kernel for non-linear decision boundaries\n",
        "}\n",
        "\n",
        "# Initialize SVM and perform Grid Search\n",
        "svm = SVC()\n",
        "grid_search = GridSearchCV(svm, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate best model\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(f\"Best Hyperparameters: {grid_search.best_params_}\")\n",
        "print(f\"Model Accuracy on Test Set: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_Wv-xcqfuAe",
        "outputId": "87f33b67-8471-4ddc-f616-19389901c2c9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "Model Accuracy on Test Set: 0.78\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 9: Write a Python program to:\n",
        "#Train a Na√Øve Bayes Classifier on a synthetic text dataset (e.g. using sklearn.datasets.fetch_20newsgroups).\n",
        "# Print the model's ROC-AUC score for its predictions.\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load a binary subset of the 20 Newsgroups dataset\n",
        "categories = ['sci.space', 'rec.sport.baseball']\n",
        "newsgroups = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "# Vectorize the text using TF-IDF\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X = vectorizer.fit_transform(newsgroups.data)\n",
        "y = newsgroups.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Multinomial Na√Øve Bayes Classifier\n",
        "nb = MultinomialNB()\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities and calculate ROC-AUC score\n",
        "y_proba = nb.predict_proba(X_test)[:, 1]\n",
        "roc_auc = roc_auc_score(y_test, y_proba)\n",
        "print(f\"ROC-AUC Score: {roc_auc:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjkzPJUkgarH",
        "outputId": "ff1c89e3-092f-42e6-e6ca-d6882e813b6a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you‚Äôre working as a data scientist for a company that handles email communications.Your task is to automatically classify emails as Spam or Not Spam. The emails maycontain:\n",
        "\n",
        "‚óè Text with diverse vocabulary\n",
        "‚óè Potential class imbalance (far more legitimate emails than spam)\n",
        "‚óè Some incomplete or missing data\n",
        "Explain the approach you would take to:\n",
        "‚óè Preprocess the data (e.g. text vectorization, handling missing data)\n",
        "‚óè Choose and justify an appropriate model (SVM vs. Na√Øve Bayes)\n",
        "‚óè Address class imbalance\n",
        "‚óè Evaluate the performance of your solution with suitable metrics\n",
        "And explain the business impact of your solution.\n",
        "\n",
        "ans- 1. Preprocessing the Data\n",
        "üîπ Handling Missing Data\n",
        "- Text fields: If email body or subject is missing, fill with \"missing\" or drop if critical.\n",
        "- Metadata (e.g., sender, timestamp): Impute with mode or flag as missing using indicator variables.\n",
        "üîπ Text Vectorization\n",
        "- Use TF-IDF Vectorizer to convert email text into numerical features:\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "X = vectorizer.fit_transform(email_texts)\n",
        "\n",
        "2. Model Selection: SVM vs. Na√Øve Bayes\n",
        "\n",
        "Model              Strengths                             Limitations\n",
        "\n",
        "Na√Øve Bayes       Fast, scalable,                       Assumes feature\n",
        "\n",
        "                   works well with sparse text data      independence\n",
        "\n",
        "  svm                 High accuracy, handles          Slower training,   \n",
        "                  high-dimensional data well           sensitive to tuning\n",
        "\n",
        "\n",
        "\n",
        "‚úÖ Recommendation:\n",
        "- Start with Multinomial Na√Øve Bayes for baseline performance.\n",
        "- Use SVM with linear or RBF kernel for refined modeling if accuracy needs improvement.\n",
        "\n",
        "‚öñÔ∏è 3. Addressing Class Imbalance\n",
        "- Resampling Techniques:\n",
        "- SMOTE (Synthetic Minority Over-sampling Technique)\n",
        "- Random undersampling of majority class\n",
        "- Class Weighting:\n",
        "- Use class_weight='balanced' in SVM or adjust priors in Na√Øve Bayes.\n",
        "- Threshold Tuning:\n",
        "- Adjust decision threshold to favor recall for spam detection.\n",
        "\n",
        "üìä 4. Performance Evaluation\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"ROC-AUC:\", roc_auc_score(y_test, model.predict_proba(X_test)[:,1]))\n",
        "\n",
        "\n",
        "\n",
        "üíº 5. Business Impact\n",
        "- Improved Productivity: Filters out spam, reducing manual email triage.\n",
        "- Enhanced Security: Detects phishing and malicious content early.\n",
        "- Customer Trust: Ensures legitimate communications aren‚Äôt lost or misclassified.\n",
        "- Scalability: Automates spam detection across millions of emails with minimal latency.\n",
        "\n"
      ],
      "metadata": {
        "id": "Taw0r84shXel"
      }
    }
  ]
}